{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e941eec7"
      },
      "source": [
        "**N-Gram**\n",
        "\n",
        "An n-gram is a contiguous sequence of n items from a given sample of text or speech. In simpler terms, it's a group of n words that appear together in a text. For example, in the sentence \"the quick brown fox\", \"the quick\", \"quick brown\", and \"brown fox\" are 2-grams (bigrams), and \"the quick brown\" and \"quick brown fox\" are 3-grams (trigrams). N-grams are commonly used in natural language processing tasks like language modeling, text prediction, and spam filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Uni-Gram**\n",
        "The most basic form of an N-gram in natural language processing (NLP) is a unigram, sometimes referred to as a 1-gram. It is made up of discrete words or tokens inside a text; each unigram represents a single word on its own, without reference to context or other words nearby."
      ],
      "metadata": {
        "id": "g2QVpuxqBdFm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPVJWQmCAlrL",
        "outputId": "0c6b9aad-c903-4cad-ab44-2900018e9a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\w'\n",
            "/tmp/ipython-input-3713890602.py:9: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  words += re.findall('\\w+',line.lower())\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "with open('big.txt','r')as fd:\n",
        "  lines = fd.readlines()\n",
        "\n",
        "  words = []\n",
        "\n",
        "  for line in lines:\n",
        "    words += re.findall('\\w+',line.lower())\n",
        "\n",
        "\n",
        "  def get_pairs(words):\n",
        "    data = []\n",
        "\n",
        "    for i in range(len(words)-1):\n",
        "      data.append(' '.join(words[i:i+2]))\n",
        "\n",
        "    return data\n",
        "\n",
        "data = get_pairs(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Probability Distributions**"
      ],
      "metadata": {
        "id": "iN-bgrz4GdAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict the next word in NLP, we harness probability distributions, particularly conditional probabilities. The process involves:\n",
        "\n",
        "1. Counting Occurrences: Just as with basic unigram pairs, we analyze a text corpus to tally the frequency of these bigrams.\n",
        "\n",
        "2. Conditional Probability: This is where the real magic happens. Given a sequence of N-1 words (in the case of bigrams, just one preceding word), probability distributions estimate the likelihood of different words occurring next.\n",
        "\n",
        "3. Using Frequency: The frequency and patterns of unigram pairs in a training corpus play a crucial role in calculating these probabilities. Words that often follow each other are assigned higher probabilities."
      ],
      "metadata": {
        "id": "AakpR1UiGrY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "rivNceRnC3LP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Occurence Probabilities"
      ],
      "metadata": {
        "id": "eNgP5srNG4ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"data\" is the words pairs we made earlier\n",
        "a = np.array(data)\n",
        "\n",
        "# use numpy to find unique pairs and their counts\n",
        "pair, count = np.unique(a, return_counts=True)\n",
        "\n",
        "print(pair)\n",
        "\n",
        "print(f'Unique pairs: {len(pair)}')\n",
        "\n",
        "print('-'*30)\n",
        "\n",
        "print('Total pairs:',len(data))\n",
        "\n",
        "unique_parts = list(set(data))\n",
        "\n",
        "print('-'*30)\n",
        "\n",
        "prob_dist = []\n",
        "\n",
        "for i in range(len(pair)):\n",
        "  prob_dist.append([pair[i], count[i], pair[i].split(' ')[-1]])\n",
        "\n",
        "print(len(prob_dist))\n",
        "\n",
        "# print(prob_dist)\n",
        "\n",
        "# In summary, this cell takes the raw list of word pairs, identifies the unique pairs and how many times each appears, and then organizes this information into a new list called prob_dist which is set up for further calculations related to probability distributions (like calculating the probability of the second word given the first).\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9JqCIEOGx8q",
        "outputId": "f7615dbf-e402-4947-a65c-62f19d1f0ea7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0 05' '0 25' '0 45' ... 'zweck ist' 'zygoma in' 'zygomatic and']\n",
            "Unique pairs: 390694\n",
            "------------------------------\n",
            "Total pairs: 1115584\n",
            "------------------------------\n",
            "390694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next word prediction with Uni-Gram**"
      ],
      "metadata": {
        "id": "CPdNvC0YI9w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the Next Word\n",
        "\n",
        "The 'predict' function within the code snippet plays a crucial role in the next word prediction. It works as follows:\n",
        "\n",
        "1. For a given input word, the function searches through the probability distribution data frame ('df') to identify uni-gram pairs that start with the input word.\n",
        "\n",
        "2. The function creates a new data frame ('df_pred') to store these identified pairs, along with their frequencies and potential next words.\n",
        "\n",
        "3. It sorts the 'df_pred' data frame by frequency in descending order, revealing which words will likely follow the input word.\n",
        "\n",
        "4. Finally, the function returns a list of the top five most probable next words."
      ],
      "metadata": {
        "id": "o_BJl1EOL8vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(prob_dist, columns=['pair', 'freq', 'out'])\n",
        "\n",
        "df = df[df['freq'] >=5]\n",
        "\n",
        "df.head()\n",
        "\n",
        "def predict(word):\n",
        "  df_pred = []\n",
        "\n",
        "  for i in df.values:\n",
        "\n",
        "    if i[0].split(' ')[0] == word:\n",
        "      # print(i)\n",
        "      df_pred.append([i[0], i[1], i[2]])\n",
        "\n",
        "  df_pred = pd.DataFrame(df_pred, columns=['in','freq','out'])\n",
        "  return list(df_pred.sort_values(by='freq', ascending=False).head()['out'].values)\n",
        "\n"
      ],
      "metadata": {
        "id": "dB-0ose-HgNF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('he')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zw_DhszNMMs",
        "outputId": "6fa8be4c-ed7e-413a-e795-b5c3b13d7ed7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['had', 'was', 'said', 'is', 'would']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Word Prediction - Auto Generated**"
      ],
      "metadata": {
        "id": "gUdRiY7PPZjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auto-Generated Sequencing**\n",
        "\n",
        "Auto-generated sequencing is the automated approach to next-word prediction. In this method, we use a starting word and iteratively predict the next word in the sequence, allowing the process to continue seamlessly. The code provided demonstrates this technique:"
      ],
      "metadata": {
        "id": "RpEtKYjAPhCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'one'\n",
        "\n",
        "for i in range(20):\n",
        "  pred = predict(word)\n",
        "  word = pred[0]\n",
        "  print(word, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVSbdYutNNWW",
        "outputId": "f7e1c991-575a-44bc-b80b-a5b6aed42c6f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "of the same time to the same time to the same time to the same time to the same time "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manual Selection**\n",
        "\n",
        "On the other hand, manual selection introduces human interaction into the prediction process. Users are presented with a list of potential next words, and they select which word they want to proceed with. The code for this manual sequencing approach is as follows:"
      ],
      "metadata": {
        "id": "ew5vmyTrP8B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'this'\n",
        "\n",
        "preds = []\n",
        "preds.append(word)\n",
        "\n",
        "for i in range(5):\n",
        "  pred = []\n",
        "\n",
        "  pred = predict(word)\n",
        "  print(pred)\n",
        "\n",
        "  word = pred[int(input(\"Enter the number: \"))]\n",
        "\n",
        "  preds.append(word)\n",
        "\n",
        "print('-'*20)\n",
        "print(' '.join(preds))\n",
        "print('-'*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cViDj1qiPxMY",
        "outputId": "e7eda119-91ae-4328-b96b-c60519be6813"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'was', 'and', 'way', 'time']\n",
            "Enter the number: 1\n",
            "['a', 'the', 'not', 'in', 'to']\n",
            "Enter the number: 1\n",
            "['same', 'french', 'first', 'old', 'emperor']\n",
            "Enter the number: 1\n",
            "['and', 'army', 'had', 'were', 'revolution']\n",
            "Enter the number: 1\n",
            "['and', 'was', 'of', 'to', 'he']\n",
            "Enter the number: 1\n",
            "--------------------\n",
            "this was the french army was\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working with Bi-Gram, Tri-Gram and N-Gram**"
      ],
      "metadata": {
        "id": "W0y0LrzPQ8DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding the Pairs: Bi-Gram, Tri-Gram, and N-Gram**\n",
        "\n",
        "The first step in text analysis involves identifying pairs of words using N-grams. The code provided focuses on extracting N-grams of varying lengths, specifically four-grams in this case. Here's how it works:"
      ],
      "metadata": {
        "id": "3uAeYf45RJro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code\n",
        "def get_pairs(words, n):\n",
        "    n = n + 1  # To consider N words as a single N-gram\n",
        "    data = []\n",
        "    for i in range(len(words) - n):\n",
        "        data.append(' '.join(words[i:i + n]))\n",
        "    return data"
      ],
      "metadata": {
        "id": "Zq9pDT7vQsjX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates N-grams by sliding a \"window\" of N words across the text, effectively creating pairs of words. For example, in the sentence \"I love natural language processing,\" a four-gram analysis would produce pairs like \"I love natural,\" \"love natural language,\" and so on."
      ],
      "metadata": {
        "id": "lcHl0Av3RdLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4xSA1tTHRd6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding Occurrence Probabilities**\n",
        "\n",
        "Once we have identified these pairs, the next step is to calculate occurrence probabilities. This is vital for understanding which words are more likely to follow others in a given context. The code for this process is as follows:"
      ],
      "metadata": {
        "id": "i3KetdNDRivr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code\n",
        "def get_prob_dist(data):\n",
        "    a = np.array(data)\n",
        "    pair, count = np.unique(a, return_counts=True)\n",
        "    unique_pairs = list(set(data))\n",
        "    prob_dist = []\n",
        "    for i in range(len(unique_pairs)):\n",
        "        prob_dist.append([unique_pairs[i], ' '.join(unique_pairs[i].split(' ')[:-1]), unique_pairs[i].split(' ')[-1], count[i]])\n",
        "    return prob_dist"
      ],
      "metadata": {
        "id": "oNs21kNiRdaC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses NumPy to process the generated N-grams and create a probability distribution. It counts the occurrences of each unique N-gram and records the individual words as well as their frequencies."
      ],
      "metadata": {
        "id": "RYUcjaFjR4WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate N-grams (four-grams in this case)\n",
        "data = get_pairs(words, 4)\n",
        "\n",
        "# Calculate occurrence probabilities for the N-grams\n",
        "prob_dist = get_prob_dist(data)"
      ],
      "metadata": {
        "id": "yi4vKtfkR4oZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Generation with Bi, Tri and N-Gram**"
      ],
      "metadata": {
        "id": "8FlEndUMSN1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting the Words**\n",
        "\n",
        "Before diving into sentence generation, we need to understand how N-gram models are used to predict the next word. The code snippet below demonstrates this process:"
      ],
      "metadata": {
        "id": "3SYjmGtzSTNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(prob_dist, columns = ['seq', 'inp', 'out', 'freq'])\n",
        "\n",
        "def predict(word):\n",
        "\n",
        "  if len(df[df['inp'] == word]):\n",
        "\n",
        "    df_ = df[df['inp'] == word]\n",
        "\n",
        "    top_predictions = df_.sort_values(by='freq').head()['out'].values\n",
        "    return top_predictions\n",
        "  else:\n",
        "    print('seq is not present')\n",
        "\n",
        "predict('this is a beautiful')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zFP0_j4R8AA",
        "outputId": "e6580aab-1c4a-4215-bdda-eabda562f951"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['country'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses a DataFrame to store N-gram sequences, input words, output words, and their frequencies. The 'predict' function predicts the next word given an input word or sequence. It checks if the input word is present in the DataFrame and returns the most probable following words based on their frequencies."
      ],
      "metadata": {
        "id": "Z21eUkK2Ta9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wEvv_vbUTf-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction with Auto Sequencing**\n",
        "\n",
        "For more extended sentence generation, the 'pred_seq' function takes a seed sequence and predicts the subsequent words to create a sentence:"
      ],
      "metadata": {
        "id": "ZCV9PGXoTffl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code\n",
        "# Function to predict a sequence of words based on an initial input sequence and a number of words to predict\n",
        "def pred_seq(seq, n):\n",
        "    output = []  # Initialize an empty list to store the predicted sequence\n",
        "    output.append(seq)  # Append the initial input sequence to the output list\n",
        "\n",
        "    for i in range(n):\n",
        "        pred = predict(seq)  # Predict the next word based on the current sequence\n",
        "        seq = ' '.join(seq.split(' ')[1:]) + ' ' + pred[0]  # Update the sequence by removing the first word and adding the predicted word\n",
        "        output.append(pred[0])  # Append the predicted word to the output list\n",
        "\n",
        "    return ' '.join(output)  # Return the generated sequence as a string\n",
        "pred_seq('of the united states', 50)\n",
        "# Example usage:\n",
        "# To generate a sequence of 5 predicted words based on an initial sequence 'apple is a fruit', you can call the function like this:\n",
        "# generated_sequence = pred_seq('apple is a fruit', 5)\n",
        "# The 'generated_sequence' variable will contain the sequence of words, including the initial input and the predicted words."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qvr8Fsw8TL-l",
        "outputId": "54ed5f29-8a26-42ce-e3e9-ba1dcdd847d6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'of the united states although it was well known had doubts about the propriety of american action in hawaii for the purpose of making an inquiry into the matter he sent a special commissioner to the islands on the basis of beginnings made five years before at anna pavlovna s an unacknowledged sense of'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7x6wTRsTnws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}